In the sixth lecture we have reviewed some fundamentals on data and information. In order to carry out successful machine learning, we need not only appropriate algorithms, but above all data. However, it is not only important to have sufficient large amounts of data, but also to have relevant quality data and the corresponding domain knowledge. You will always get a result, the crucial question is whether and to what extent the results are relevant to support medical decision making from uncertainty; and here we need the concept of Bayes and Laplace and entropy as a measure of uncertainty distributions, and  KL divergence as a way of measuring the matching between two distributions.

Lecture Keywords: data, information, probability, entropy, cross-entropy, Kullback-Leibler divergence

Topic 01 Data – The underlying physics of data
Topic 02 Data – Biomedical data sources – taxonomy of data
Topic 03 Data – Integration, Mapping and Fusion of data
Topic 04 Information  – Bayes and Laplace probabilistic information p(x)
Topic 05 Information Theory – Information Entropy
Topic 06 Information Cross-Entropy and Kullback-Leibler Divergence

Data Quality is of utmost importance. 

https://hci-kdd.org/machine-learning-for-health-informatics-class-2019
